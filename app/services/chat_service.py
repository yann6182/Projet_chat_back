from typing import Dict, List, Optional
import uuid
import time
import logging
from collections import OrderedDict
from sqlalchemy.orm import Session
from app.schemas.chat import ChatRequest, ChatResponse, Excerpt
from app.services.retrieval_service import RetrievalService
from app.services.embedding_service import EmbeddingService
from app.services.chroma_service import ChromaService  # Import du nouveau service ChromaDB
from app.services.document_generator_service import DocumentGeneratorService  # Import du service de g√©n√©ration de documents
from app.models.model import Conversation, Question, Response, User
from app.db.database import SessionLocal
from mistralai.client import MistralClient
import os
from fastapi import HTTPException
import json
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class LRUCache:
    def __init__(self, capacity: int):
        self.cache = OrderedDict()
        self.capacity = capacity

    def get(self, key):
        if key in self.cache:
            self.cache.move_to_end(key)
            return self.cache[key]
        return None

    def put(self, key, value):
        if key in self.cache:
            self.cache.move_to_end(key)
        self.cache[key] = value
        if len(self.cache) > self.capacity:
            self.cache.popitem(last=False)

    def __contains__(self, key):
        return key in self.cache

    def delete(self, key):
        if key in self.cache:
            del self.cache[key]
            return True
        return False

class ChatService:
    def __init__(self, 
                 model_name: str = "mistral-large-latest", 
                 max_conversations: int = 1000,
                 conversation_ttl: int = 3600):
        self.retrieval_service = RetrievalService()
        
        # Initialisation des services d'embedding et de recherche vectorielle
        try:
            self.embedding_service = EmbeddingService()
            # Initialiser √©galement le service ChromaDB
            self.chroma_service = ChromaService()
            self.use_chroma = True  # Utiliser ChromaDB par d√©faut si disponible
            logger.info("‚úÖ Services d'embedding et ChromaDB initialis√©s avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation des services RAG: {str(e)}")
            logger.warning("‚ö†Ô∏è Le service RAG sera d√©sactiv√©")
            self.embedding_service = None
            self.chroma_service = None
            self.use_chroma = False
            
        # Initialiser le service de g√©n√©ration de documents
        try:
            self.document_generator = DocumentGeneratorService()
            logger.info("‚úÖ Service de g√©n√©ration de documents initialis√© avec succ√®s")
        except Exception as e:
            logger.error(f"‚ùå Erreur lors de l'initialisation du service de g√©n√©ration de documents: {str(e)}")
            logger.warning("‚ö†Ô∏è Le service de g√©n√©ration de documents sera d√©sactiv√©")
            self.document_generator = None
            
        self.conversations = LRUCache(max_conversations)
        self.timestamps = {}
        self.conversation_ttl = conversation_ttl
        self.max_history_messages = 5
        self.max_output_tokens = 200

        try:
            # Utiliser directement le chemin absolu vers le fichier .env
            from dotenv import load_dotenv
            import os
            
            # Chargement explicite du fichier .env
            env_path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), 'app', '.env')
            load_dotenv(env_path)
            
            # R√©cup√©rer la cl√© API apr√®s chargement
            mistral_api_key = os.getenv("MISTRAL_API_KEY")
            if not mistral_api_key:
                logger.error(f"Cl√© API Mistral non trouv√©e dans {env_path}")
                raise ValueError("Cl√© API Mistral non trouv√©e")
            
            # Afficher la cl√© partiellement pour debug (premiers et derniers caract√®res)
            key_preview = mistral_api_key[:4] + "..." + mistral_api_key[-4:]
            logger.info(f"Cl√© API Mistral trouv√©e: {key_preview}")
            
            self.client = MistralClient(api_key=mistral_api_key)
            self.model = model_name
            if os.path.exists(self.embedding_service.index_path):
                self.embedding_service.load_index()
            logger.info("Client Mistral API (v1) initialis√© avec base vectorielle charg√©e")
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation du client Mistral: {str(e)}")
            raise
            
    def _generate_title(self, query: str) -> str:
        """
        G√©n√®re un titre pour la conversation bas√© sur la requ√™te initiale.
        """
        max_length = 50  # Limite de caract√®res pour le titre
        title = query.strip().capitalize()
        if len(title) > max_length:
            title = title[:max_length].rsplit(' ', 1)[0] + "..."
        return title
        
    async def process_query(self, request: ChatRequest, conversation_id: str, user_id: int) -> ChatResponse:
        """
        Traite une requ√™te utilisateur et g√©n√®re une r√©ponse en utilisant l'architecture RAG.
        
        Args:
            request: La requ√™te utilisateur
            conversation_id: L'identifiant de la conversation
            user_id: L'identifiant de l'utilisateur
            
        Returns:
            La r√©ponse structur√©e avec contexte et sources
        """        # D√©terminer la cat√©gorie et nettoyer les conversations expir√©es
        category = self._determine_category(request.query)
        self._cleanup_expired_conversations()
        
        # R√©cup√©rer l'historique ou initialiser une nouvelle conversation
        conversation_history = self.conversations.get(conversation_id) if conversation_id in self.conversations else []
        self.timestamps[conversation_id] = time.time()
        
        try:
            # Collecter tous les documents pertinents (vectoriels + contexte fourni)
            all_relevant_documents = []
            sources = []
            has_relevant_docs = False  # Variable pour suivre si des documents pertinents ont √©t√© trouv√©s
            
            # 1. Ajouter les documents contextuels fournis par le front-end, s'il y en a
            if hasattr(request, 'context_documents') and request.context_documents:
                logger.info(f"Documents contextuels fournis par le front-end: {len(request.context_documents)}")
                for i, doc in enumerate(request.context_documents):
                    # Convertir en dictionnaire pour le format compatible avec les autres documents
                    all_relevant_documents.append({
                        'content': doc.content,
                        'source': f"Document fourni: {doc.source}",
                        'page': doc.page,
                        'score': 0.0  # Score artificiel pour prioriser ces documents
                    })
                    sources.append(f"Document fourni: {doc.source}")
              # 2. üîç √âtape RAG : recherche de documents similaires dans la base vectorielle
            # Utiliser ChromaDB si disponible, sinon fallback sur l'ancien service d'embedding
            if self.use_chroma and hasattr(self, 'chroma_service') and self.chroma_service:
                try:
                    logger.info("üîç Recherche de documents similaires dans ChromaDB")
                    vector_documents = self.chroma_service.search(
                        query=request.query, 
                        k=3,  # R√©cup√©rer les 3 documents les plus pertinents
                        threshold=0.25  # Filtrer les documents peu pertinents
                    )
                    # Ajouter les documents de ChromaDB
                    all_relevant_documents.extend(vector_documents)
                    
                    # Ajouter les sources
                    for doc in vector_documents:
                        source_info = doc['source']
                        if doc.get('page'):
                            source_info += f" (page {doc['page']})"
                        if source_info not in sources:
                            sources.append(source_info)
                    
                    logger.info(f"‚úÖ {len(vector_documents)} documents pertinents trouv√©s dans ChromaDB")
                            
                except Exception as e:
                    logger.warning(f"‚ùå Impossible d'effectuer la recherche ChromaDB: {str(e)}")
                    # Fallback sur l'ancien service d'embedding si disponible
                    if hasattr(self, 'embedding_service') and self.embedding_service:
                        try:
                            logger.info("üîÑ Utilisation du service d'embedding de secours (FAISS)")
                            vector_documents = self.embedding_service.search(
                                query=request.query, 
                                k=3,
                                threshold=0.25
                            )
                            all_relevant_documents.extend(vector_documents)
                            
                            # Ajouter les sources
                            for doc in vector_documents:
                                source_info = doc['source']
                                if doc.get('page'):
                                    source_info += f" (page {doc['page']})"
                                if source_info not in sources:
                                    sources.append(source_info)
                        except Exception as e:
                            logger.warning(f"‚ùå Impossible d'effectuer la recherche RAG avec FAISS: {str(e)}")
            elif hasattr(self, 'embedding_service') and self.embedding_service:
                # Utiliser l'ancien service d'embedding si ChromaDB n'est pas disponible
                try:
                    logger.info("üîÑ Utilisation du service d'embedding FAISS")
                    vector_documents = self.embedding_service.search(
                        query=request.query, 
                        k=3,  # R√©cup√©rer les 3 documents les plus pertinents
                        threshold=0.25  # Filtrer les documents peu pertinents
                    )
                    # Ajouter les documents de la base vectorielle
                    all_relevant_documents.extend(vector_documents)
                    
                    # Ajouter les sources
                    for doc in vector_documents:
                        source_info = doc['source']
                        if doc.get('page'):
                            source_info += f" (page {doc['page']})"
                        if source_info not in sources:
                            sources.append(source_info)
                            
                except Exception as e:
                    logger.warning(f"‚ùå Impossible d'effectuer la recherche RAG: {str(e)}")
              # 3. üîç √âtape ChromaDB : recherche de documents similaires dans ChromaDB, si activ√©
            if self.use_chroma and hasattr(self, 'chroma_service'):
                try:
                    chroma_documents = self.chroma_service.search(
                        query=request.query,
                        k=3,  # R√©cup√©rer les 3 documents les plus pertinents
                        filter_criteria=None  # Aucun filtre suppl√©mentaire pour l'instant
                    )
                    
                    logger.info(f"Documents trouv√©s dans ChromaDB: {len(chroma_documents)}")
                    
                    # Ajouter les documents de ChromaDB
                    all_relevant_documents.extend(chroma_documents)
                    
                    # Ajouter les sources
                    for doc in chroma_documents:
                        source_info = doc['source']
                        if doc.get('page'):
                            source_info += f" (page {doc['page']})"
                        if source_info not in sources:
                            sources.append(source_info)
                            
                except Exception as e:
                    logger.warning(f"Impossible d'effectuer la recherche dans ChromaDB: {str(e)}")
                # Enrichissement du contexte avec tous les documents pertinents
            context = ""
            # Pr√©parer la liste des extraits utilis√©s pour la r√©ponse
            excerpts = []
            # √âvaluer la pertinence des documents trouv√©s
            relevant_docs = []
            has_relevant_docs = False
            
            # Ignorer les documents pour les questions tr√®s g√©n√©rales ou sans rapport avec le domaine juridique
            simple_questions = ["ca va", "√ßa va", "comment vas-tu", "bonjour", "salut", "hello", "coucou"]
            if any(simple_q in request.query.lower() for simple_q in simple_questions) and len(request.query) < 20:
                # Si la requ√™te est une question tr√®s simple/g√©n√©rale, ne pas inclure de sources
                logger.info("Question g√©n√©rale d√©tect√©e, pas de sources n√©cessaires")
                all_relevant_documents = []
            
            if all_relevant_documents:# Filtrer les documents avec un score de pertinence √©lev√©                # Exclure compl√®tement les questions tr√®s g√©n√©rales
                simple_questions = ["ca va", "√ßa va", "comment vas-tu", "bonjour", "salut", "hello", "coucou"]
                is_simple_question = any(simple_q in request.query.lower() for simple_q in simple_questions) and len(request.query) < 20
                
                if not is_simple_question:
                    for doc in all_relevant_documents:
                        # Consid√©rer les documents fournis comme toujours pertinents
                        if 'score' in doc and doc['score'] > 0.7:  # Seuil encore plus √©lev√© pour √©viter les faux positifs
                            relevant_docs.append(doc)
                            has_relevant_docs = True
                        elif doc.get('source', '').startswith('Document fourni'):
                            # Les documents fournis par l'utilisateur sont toujours consid√©r√©s comme pertinents
                            relevant_docs.append(doc)
                            has_relevant_docs = True
                
                if has_relevant_docs:
                    context = "Contexte juridique pertinent:\n\n"
                    sources = []  # R√©initialiser les sources pour ne garder que les pertinentes
                    
                    for i, doc in enumerate(relevant_docs, 1):
                        # Limiter la taille des extraits pour √©viter de d√©passer le contexte
                        excerpt = doc['content']
                        if len(excerpt) > 800:  # Limiter √† 800 caract√®res par extrait
                            excerpt = excerpt[:800] + "..."
    
                        # Inclure la source et le num√©ro de page si disponible
                        source_info = doc['source']
                        page = doc.get('page')
                        if page:
                            source_info += f" (page {page})"
    
                        # Ajouter l'extrait au contexte
                        context += f"Document {i}: {excerpt}\nSource: {source_info}\n\n"
    
                        # Collecter les sources pour la r√©ponse
                        if source_info not in sources:
                            sources.append(source_info)
    
                        # Ajouter √† la liste des extraits pour l'API
                        excerpts.append({
                            "content": excerpt,
                            "source": doc['source'],
                            "page": doc.get('page')
                        })
                else:
                    # Aucun document pertinent malgr√© la recherche
                    logger.info("üîç Documents trouv√©s mais pas assez pertinents pour la requ√™te")
                    context = ""
                    sources = []
                    excerpts = []
            else:
                context = ""
                sources = []
                excerpts = []

            # G√©n√©ration de la r√©ponse avec le contexte enrichi
            answer = await self._generate_response(request.query, conversation_history, context)
            
            # Mise √† jour de l'historique de conversation
            conversation_history.append({"role": "user", "message": request.query})
            conversation_history.append({"role": "assistant", "message": answer})            # Limitation de la taille de l'historique
            if len(conversation_history) > self.max_history_messages * 2:
                conversation_history = conversation_history[-self.max_history_messages * 2:]
                
            # Mise en cache de la conversation
            self.conversations.put(conversation_id, conversation_history)
            
            # üíæ Enregistrer en base de donn√©es
            db: Session = SessionLocal()
            try:
                # R√©cup√©rer la conversation existante
                db_conversation = db.query(Conversation).filter_by(uuid=conversation_id).first()
                if not db_conversation:
                    logger.error(f"Conversation non trouv√©e pour conversation_id={conversation_id}")
                    raise HTTPException(status_code=404, detail="Conversation non trouv√©e.")
                
                # Ajouter la question
                db_question = Question(question_text=request.query, conversation_id=db_conversation.id)
                db.add(db_question)
                db.commit()
                db.refresh(db_question)
                  # Ajouter la r√©ponse avec les donn√©es RAG (sources et extraits)
                db_response = Response(
                    response_text=answer,
                    conversation_id=db_conversation.id,
                    question_id=db_question.id,
                    sources=sources if sources else None,  # Liste des sources
                    excerpts=excerpts if excerpts else None  # Les extraits sont d√©j√† au format dict
                )
                db.add(db_response)
                db.commit()
                  # V√©rifier si c'est le premier √©change (1 question + 1 r√©ponse)
                # et mettre √† jour le titre et la cat√©gorie intelligemment
                question_count = db.query(Question).filter(
                    Question.conversation_id == db_conversation.id
                ).count()
                
                if question_count == 1:
                    logger.info(f"Premier √©change d√©tect√© pour la conversation {conversation_id}, mise √† jour des m√©tadonn√©es...")
                    await self.update_conversation_metadata(conversation_id, db)
            finally:
                db.close()
                
            # D√©tecter si l'utilisateur demande explicitement un document
            doc_detection = self._detect_document_request(request.query)
            generated_document = None
            
            # Si c'est une demande de document et que le service de g√©n√©ration est disponible
            if doc_detection["is_doc_request"] and hasattr(self, 'document_generator') and self.document_generator:
                try:
                    # Ouvrir une nouvelle session DB pour r√©cup√©rer les informations de la conversation
                    doc_db = SessionLocal()
                    try:
                        db_conversation = doc_db.query(Conversation).filter_by(uuid=conversation_id).first()
                        if db_conversation:
                            # D√©terminer le format (PDF par d√©faut si non sp√©cifi√©)
                            doc_format = doc_detection["format"] or "pdf"
                            
                            # G√©n√©rer le titre du document
                            doc_title = f"R√©ponse √†: {request.query[:50]}" if len(request.query) > 50 else f"R√©ponse √†: {request.query}"
                            
                            # Pr√©parer le contenu du document
                            doc_content = f"Question: {request.query}\n\nR√©ponse: {answer}"
                            
                            # G√©n√©rer les m√©tadonn√©es
                            metadata = {
                                "Date de g√©n√©ration": datetime.now().strftime("%d/%m/%Y %H:%M"),
                                "ID de conversation": conversation_id,
                                "Type": "R√©ponse automatique"
                            }
                            
                            # G√©n√©rer le document selon le format demand√©
                            if doc_format.lower() == "pdf":
                                file_path = self.document_generator.generate_pdf(
                                    title=doc_title, 
                                    content=doc_content, 
                                    metadata=metadata,
                                    sources=sources if sources and has_relevant_docs else None
                                )
                            else:  # docx
                                file_path = self.document_generator.generate_word(
                                    title=doc_title, 
                                    content=doc_content, 
                                    metadata=metadata,
                                    sources=sources if sources and has_relevant_docs else None
                                )
                            
                            # R√©cup√©rer le nom du fichier
                            filename = os.path.basename(file_path)
                            
                            # Cr√©er l'info du document g√©n√©r√©
                            generated_document = {
                                "filename": filename,
                                "url": f"/api/document-generator/download/{filename}",
                                "format": doc_format
                            }
                            
                            # Ajouter une note √† la r√©ponse concernant le document
                            answer += f"\n\nJ'ai √©galement g√©n√©r√© un document {doc_format.upper()} avec cette r√©ponse que vous pouvez t√©l√©charger."
                            
                            logger.info(f"Document {doc_format} g√©n√©r√© automatiquement: {filename}")
                    finally:
                        doc_db.close()
                        
                except Exception as e:
                    logger.error(f"Erreur lors de la g√©n√©ration automatique du document: {str(e)}")
                    answer += "\n\nD√©sol√©, je n'ai pas pu g√©n√©rer le document demand√© suite √† une erreur technique."
            
            # Pr√©parer la r√©ponse avec ou sans document g√©n√©r√©
            from app.schemas.chat import DocumentInfo
            doc_info = None
            if generated_document:
                doc_info = DocumentInfo(
                    filename=generated_document["filename"],
                    url=generated_document["url"],
                    format=generated_document["format"]
                )
            
            # Ne retourner les sources et extraits que s'ils sont pertinents
            if has_relevant_docs:
                return ChatResponse(
                    answer=answer,
                    sources=sources,
                    conversation_id=conversation_id,
                    excerpts=excerpts,
                    generated_document=doc_info
                )
            else:
                # Ne pas inclure de sources ou extraits si aucun document pertinent n'a √©t√© trouv√©
                return ChatResponse(
                    answer=answer,
                    sources=[],  # Pas de sources √† afficher
                    conversation_id=conversation_id,
                    excerpts=[],  # Pas d'extraits √† afficher
                    generated_document=doc_info
                )

        except Exception as e:
            logger.error(f"Erreur lors du traitement de la requ√™te: {str(e)}", exc_info=True)
            # Toujours retourner une cha√Æne pour conversation_id (jamais None)
            safe_conversation_id = conversation_id if conversation_id else ""
            return ChatResponse(
                answer="Je suis d√©sol√©, une erreur s'est produite lors du traitement de votre demande.",
                sources=[],
                conversation_id=safe_conversation_id
            )
            
    def _determine_category(self, query: str) -> str:
        """
        D√©termine la cat√©gorie de la question bas√©e sur des mots-cl√©s.
        """
        query = query.lower()
        
        # D√©finition des cat√©gories et mots-cl√©s associ√©s
        category_keywords = {
            'treasury': ['tr√©sorerie', 'finance', 'budget', 'financial', 'comptable', 'accounting', 'tva'],
            'organisational': ['structure', 'organisation', 'management', 'r√©union', '√©quipe', 'team', 'gestion'],
            'other': ['juridique', 'legal', 'g√©n√©ral', 'question', 'help', 'aide']
        }
        
        # V√©rifier les mots-cl√©s dans la requ√™te
        for category, keywords in category_keywords.items():
            for keyword in keywords:
                if keyword in query:
                    return category
                    
        return "other"  # Cat√©gorie par d√©faut
    async def _generate_response(self, query: str, conversation_history: List[Dict], context: str = "") -> str:
        """
        G√©n√®re une r√©ponse bas√©e sur la requ√™te, l'historique et le contexte RAG.
        Utilise une structure de prompt optimis√©e pour maximiser la pertinence du contexte juridique.
        
        Args:
            query: La question de l'utilisateur
            conversation_history: L'historique de la conversation
            context: Le contexte extrait de la base de connaissances (documents pertinents)
            
        Returns:
            La r√©ponse g√©n√©r√©e par le mod√®le
        """
        try:
            # Pr√©parer l'historique de conversation au format attendu par l'API
            history_messages = []
            
            # Ne garder que les 6 derniers messages pour √©viter de d√©passer le contexte
            for msg in conversation_history[-6:]:
                # Conversion du format interne au format API
                history_messages.append({
                    "role": msg["role"], 
                    "content": msg.get("message", msg.get("content", ""))
                })
              # Construction du syst√®me prompt optimis√© pour le contexte juridique des Junior-Entreprises
            system_prompt = """Tu es un assistant juridique sp√©cialis√© pour les Junior-Entreprises en France.
Tu fais preuve de pr√©cision, de clart√© et de p√©dagogie dans tes r√©ponses.

DIRECTIVES IMPORTANTES:
1. Si un contexte juridique est fourni, utilise UNIQUEMENT ces informations pour √©laborer ta r√©ponse
2. Si le contexte ne contient pas suffisamment d'informations pour r√©pondre √† la question, indique-le clairement
3. Cite pr√©cis√©ment tes sources (document, page, article, texte de loi, etc.) UNIQUEMENT quand tu utilises le contexte fourni
4. N'invente JAMAIS de r√©f√©rences juridiques ou de r√®glements qui ne seraient pas mentionn√©s explicitement dans le contexte
5. Pr√©sente les informations de fa√ßon structur√©e avec des paragraphes courts et des puces lorsque c'est pertinent
6. Exprime-toi dans un fran√ßais clair, pr√©cis et accessible, en √©vitant le jargon juridique complexe
7. Lorsque tu cites des extraits du contexte, indique clairement qu'il s'agit de citations
8. Si AUCUN contexte n'est fourni, r√©ponds de mani√®re g√©n√©rale sans inventer de r√©f√©rences juridiques sp√©cifiques

Tu dois √™tre une aide pr√©cieuse pour les responsables de Junior-Entreprises qui ont besoin d'informations juridiques fiables."""            # Int√©gration optimis√©e du contexte RAG dans le prompt utilisateur
            if context:
                user_prompt = f"""En te basant UNIQUEMENT sur les informations juridiques suivantes:

{context}

R√©ponds √† ma question de mani√®re structur√©e et pr√©cise. N'h√©site pas √† citer des extraits pertinents du contexte pour appuyer ton propos.
Si les informations fournies ne sont pas suffisamment pertinentes pour r√©pondre √† ma question, indique-le clairement.

Ma question: {query}"""
            else:
                user_prompt = f"""Ma question est: {query}

Je comprends que tu n'as pas de contexte juridique sp√©cifique pour r√©pondre √† cette question. 
R√©ponds de la fa√ßon la plus pr√©cise possible en fonction des connaissances g√©n√©rales dont tu disposes, sans citer de sources sp√©cifiques.
Si cette question n√©cessite des informations juridiques sp√©cialis√©es que tu ne poss√®des pas, indique-le clairement."""
            
            # Appel √† l'API Mistral avec le prompt structur√©
            chat_response = self.client.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": system_prompt},
                    *history_messages,
                    {"role": "user", "content": user_prompt}
                ]
            )
            
            return chat_response.choices[0].message.content
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration de r√©ponse avec Mistral API: {str(e)}", exc_info=True)
            return "Je suis d√©sol√©, je ne peux pas g√©n√©rer de r√©ponse pour le moment."
            
    def _cleanup_expired_conversations(self):
        current_time = time.time()
        expired_ids = [
            conv_id for conv_id, timestamp in self.timestamps.items()
            if current_time - timestamp > self.conversation_ttl
        ]
        for conv_id in expired_ids:
            self.clear_conversation(conv_id)
            if conv_id in self.timestamps:
                del self.timestamps[conv_id]
                
    def get_conversation_history(self, conversation_id: str) -> List[dict]:
        """
        R√©cup√®re l'historique complet d'une conversation √† partir de son ID,
        y compris les donn√©es RAG (sources et extraits).
        """
        if not conversation_id:
            return []
        
        # Log pour le d√©bogage
        print(f"R√©cup√©ration de l'historique pour conversation_id: {conversation_id}")
        
        db = SessionLocal()
        try:
            # V√©rifier si la conversation existe
            conversation = db.query(Conversation).filter(Conversation.uuid == conversation_id).first()
            if not conversation:
                print(f"Conversation {conversation_id} non trouv√©e en base")
                return []
                
            # R√©cup√©rer les questions et r√©ponses li√©es √† cette conversation
            from app.models.model import Question, Response
            
            # R√©cup√©rer toutes les questions pour cette conversation
            questions = db.query(Question).filter(
                Question.conversation_id == conversation.id
            ).order_by(Question.created_at).all()
            
            history = []
            
            for question in questions:
                # R√©cup√©rer la r√©ponse associ√©e √† cette question
                response = db.query(Response).filter(
                    Response.question_id == question.id
                ).first()
                
                history.append({
                    "role": "user",
                    "content": question.question_text,
                    "timestamp": question.created_at.isoformat() if hasattr(question, 'created_at') else None
                })
                
                if response:
                    # Inclure les donn√©es RAG si disponibles
                    response_data = {
                        "role": "assistant",
                        "content": response.response_text,
                        "timestamp": response.created_at.isoformat() if hasattr(response, 'created_at') else None
                    }
                    
                    # Ajouter les sources si disponibles
                    if hasattr(response, 'sources') and response.sources:
                        response_data["sources"] = response.sources
                    
                    # Ajouter les extraits si disponibles
                    if hasattr(response, 'excerpts') and response.excerpts:
                        response_data["excerpts"] = response.excerpts
                    
                    history.append(response_data)
            
            print(f"Historique r√©cup√©r√©: {len(history)} messages")
            return history
            
        except Exception as e:
            print(f"Erreur lors de la r√©cup√©ration de l'historique: {str(e)}")
            import traceback
            traceback.print_exc()
            return []
        finally:
            db.close()

    def clear_conversation(self, conversation_id: str) -> bool:
        success = self.conversations.delete(conversation_id)
        if conversation_id in self.timestamps:
            del self.timestamps[conversation_id]
        return success
    
    async def generate_smart_title(self, query: str, response: str) -> dict:
        """
        G√©n√®re un titre intelligent pour la conversation bas√© sur la premi√®re question et r√©ponse.
        Utilise le LLM pour cr√©er un titre pertinent et d√©terminer la cat√©gorie.
        
        Args:
            query: La premi√®re question de l'utilisateur
            response: La premi√®re r√©ponse du syst√®me
            
        Returns:
            Un dictionnaire contenant le titre et la cat√©gorie
        """
        try:
            # Liste des cat√©gories disponibles
            categories = ["treasury", "organisational", "legal", "general", "other"]
            category_descriptions = {
                "treasury": "Finance, comptabilit√©, budget, TVA, tr√©sorerie",
                "organisational": "Structure, organisation, management, √©quipe, gestion",
                "legal": "Juridique, l√©gal, contrats, r√®glements",
                "general": "Questions g√©n√©rales sur l'entreprise",
                "other": "Autres sujets"
            }
            
            # Construction du prompt pour le LLM
            prompt = f"""Analyse la question suivante et sa r√©ponse, puis:
1. G√©n√®re un titre concis en fran√ßais (maximum 50 caract√®res) qui r√©sume bien le sujet
2. Classe cette conversation dans une de ces cat√©gories: {', '.join(categories)}

Descriptions des cat√©gories:
- treasury: {category_descriptions["treasury"]}
- organisational: {category_descriptions["organisational"]}
- legal: {category_descriptions["legal"]}
- general: {category_descriptions["general"]}
- other: {category_descriptions["other"]}

Question: {query}

R√©ponse: {response}

R√©ponds exactement au format JSON suivant:
{{
  "title": "Titre concis",
  "category": "cat√©gorie choisie"
}}
"""
            # Appel au LLM pour g√©n√©rer le titre et la cat√©gorie
            chat_response = self.client.chat(
                model=self.model,
                messages=[
                    {"role": "system", "content": "Tu es un assistant qui analyse des conversations et g√©n√®re des titres pertinents."},
                    {"role": "user", "content": prompt}
                ]
            )
            
            # Extraire la r√©ponse du mod√®le
            result_text = chat_response.choices[0].message.content
            
            try:
                # Extraire le JSON de la r√©ponse
                import json
                import re
                
                # Chercher un objet JSON dans la r√©ponse
                json_match = re.search(r'({[\s\S]*})', result_text)
                if json_match:
                    result_json = json.loads(json_match.group(1))
                else:
                    # Fallback si le format n'est pas respect√©
                    logger.warning("Format JSON non respect√© dans la r√©ponse du LLM")
                    return {
                        "title": self._generate_title(query),  # Utiliser la m√©thode simple comme fallback
                        "category": self._determine_category(query)
                    }
                
                # Valider les champs requis
                if "title" not in result_json or "category" not in result_json:
                    raise ValueError("Les champs title et category sont requis dans la r√©ponse")
                
                # V√©rifier que la cat√©gorie est valide
                if result_json["category"] not in categories:
                    result_json["category"] = "other"
                
                # Limiter la taille du titre si n√©cessaire
                if len(result_json["title"]) > 50:
                    result_json["title"] = result_json["title"][:47] + "..."
                
                logger.info(f"Titre g√©n√©r√©: {result_json['title']}, Cat√©gorie: {result_json['category']}")
                return result_json
                
            except Exception as e:
                logger.error(f"Erreur de parsing du JSON: {str(e)}")
                # En cas d'erreur, utiliser les m√©thodes simples comme fallback
                return {
                    "title": self._generate_title(query),
                    "category": self._determine_category(query)
                }
                
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration du titre intelligent: {str(e)}")
            # En cas d'erreur, utiliser les m√©thodes simples comme fallback
            return {
                "title": self._generate_title(query),
                "category": self._determine_category(query)
            }
    
    async def update_conversation_metadata(self, conversation_id: str, db: Session) -> bool:
        """
        Met √† jour les m√©tadonn√©es (titre et cat√©gorie) d'une conversation apr√®s le premier √©change.
        
        Args:
            conversation_id: L'identifiant de la conversation
            db: Session de base de donn√©es
            
        Returns:
            True si la mise √† jour a r√©ussi, False sinon
        """
        try:
            # R√©cup√©rer la conversation
            conversation = db.query(Conversation).filter(Conversation.uuid == conversation_id).first()
            if not conversation:
                logger.warning(f"Conversation non trouv√©e pour l'ID {conversation_id}")
                return False
            
            # R√©cup√©rer la premi√®re question et la premi√®re r√©ponse
            first_question = db.query(Question).filter(
                Question.conversation_id == conversation.id
            ).order_by(Question.created_at).first()
            
            if not first_question:
                logger.warning(f"Aucune question trouv√©e pour la conversation {conversation_id}")
                return False
            
            first_response = db.query(Response).filter(
                Response.question_id == first_question.id
            ).first()
            
            if not first_response:
                logger.warning(f"Aucune r√©ponse trouv√©e pour la premi√®re question de la conversation {conversation_id}")
                return False
            
            # G√©n√©rer un titre intelligent et une cat√©gorie
            metadata = await self.generate_smart_title(
                query=first_question.question_text,
                response=first_response.response_text
            )
            
            # Mettre √† jour la conversation
            conversation.title = metadata["title"]
            conversation.category = metadata["category"]
            conversation.updated_at = datetime.utcnow()
            
            # Enregistrer les modifications
            db.commit()
            
            logger.info(f"M√©tadonn√©es mises √† jour pour la conversation {conversation_id}: {metadata}")
            return True
            
        except Exception as e:
            logger.error(f"Erreur lors de la mise √† jour des m√©tadonn√©es: {str(e)}")
            return False
    def _detect_document_request(self, query: str) -> dict:
        """
        D√©tecte si la requ√™te de l'utilisateur concerne la g√©n√©ration d'un document.
        
        Args:
            query: La requ√™te de l'utilisateur
            
        Returns:
            Un dictionnaire avec les informations sur la d√©tection :
                - is_doc_request: True si la requ√™te concerne un document
                - format: 'pdf' ou 'docx' si sp√©cifi√©, None sinon
        """
        # Normaliser la requ√™te pour la recherche
        query_lower = query.lower().strip()
        
        # Mots cl√©s pour d√©tecter une demande de document (plus complets et vari√©s)
        doc_keywords = [
            # Demandes directes
            "g√©n√©rer un document", "generer un document", "g√©n√®re un document", "genere un document",
            "cr√©e un document", "cree un document", "cr√©er un document", "creer un document",
            "faire un document", "produire un document", "exporter en", "exporte en",
            "g√©n√©rer un pdf", "generer un pdf", "g√©n√®re un pdf", "genere un pdf",
            "g√©n√©rer un word", "generer un word", "g√©n√®re un word", "genere un word",
            
            # Formats sp√©cifiques
            "en format", "au format", "en pdf", "en word", "document pdf", "document word",
            "fichier pdf", "fichier word", "rapport pdf", "rapport word",
            
            # Tournures vari√©es
            "sous forme de document", "sous forme de pdf", "sous forme de word",
            "sous forme d'un document", "sous forme d'un pdf", "sous forme d'un word",
            "me donner un document", "me fournir un document", "me donner un pdf", "me fournir un pdf",
            "me donner un word", "me fournir un word", "m'envoyer un document", "m'envoyer un pdf",
            "t√©l√©charger un document", "telecharger un document", "t√©l√©charger un pdf", "telecharger un pdf",
            
            # Expressions plus complexes
            "je voudrais un document", "j'aimerais un document", "je souhaiterais un document", 
            "je voudrais un pdf", "j'aimerais un pdf", "je souhaiterais un pdf",
            "peux-tu me faire un document", "peux-tu me faire un pdf", "peux-tu me faire un word",
            "peux-tu g√©n√©rer un document", "peux-tu generer un document",
            "pourrais-tu me faire un document", "pourrais-tu g√©n√©rer un document",
            
            # Demandes sp√©cifiques
            "version pdf", "version word", "convertir en pdf", "convertir en word",
            "r√©ponse en pdf", "reponse en pdf", "r√©ponse en document", "reponse en document"
        ]
        
        # Expressions plus longues et compl√®tes
        doc_phrases = [
            "je veux ta r√©ponse en pdf",
            "je veux ta r√©ponse en word",
            "je veux cette r√©ponse en pdf",
            "je veux cette r√©ponse en word",
            "peut-on avoir cette r√©ponse sous forme de document",
            "peut-on avoir cette r√©ponse sous forme de pdf",
            "peut-on avoir cette r√©ponse sous forme de word",
            "g√©n√®re-moi un document avec cette r√©ponse",
            "g√©n√®re-moi un pdf avec cette r√©ponse",
            "g√©n√®re-moi un word avec cette r√©ponse",
            "transforme ta r√©ponse en document",
            "transforme ta r√©ponse en pdf",
            "transforme ta r√©ponse en word",
            "donne-moi ta r√©ponse dans un document",
            "donne-moi ta r√©ponse dans un pdf",
            "donne-moi ta r√©ponse dans un word"
        ]
        
        # V√©rifier si la requ√™te contient un mot cl√© de document ou une phrase compl√®te
        is_doc_request = any(keyword in query_lower for keyword in doc_keywords) or \
                        any(phrase in query_lower for phrase in doc_phrases)
        
        # V√©rifier les expressions de d√©but ou fin de phrase
        doc_starts = [
            "document", "pdf", "word", "docx", 
            "faire un", "g√©n√©rer un", "generer un", "cr√©er un", "creer un"
        ]
        
        if not is_doc_request:
            # V√©rifier si la requ√™te commence ou se termine par certains mots cl√©s
            is_doc_request = query_lower.startswith(tuple(doc_starts)) or \
                            query_lower.endswith(("en pdf", "en word", "en document", "en docx", "en doc"))
        
        # Analyse avanc√©e: rechercher des structures de phrases typiques
        if not is_doc_request:
            import re
            # Patterns comme "... en format pdf", "... sous forme de document", etc.
            doc_patterns = [
                r"en\s+(?:format|forme(?:\s+de)?)(?:\s+de)?\s+(?:document|pdf|word|docx|doc)",
                r"sous\s+(?:format|forme)(?:\s+d[e'](?:un)?)?\s+(?:document|pdf|word|docx|doc)",
                r"(?:g√©n√©rer|generer|cr√©er|creer|produire|faire)(?:\s+un)?\s+(?:document|pdf|word|docx|doc)"
            ]
            
            is_doc_request = any(re.search(pattern, query_lower) for pattern in doc_patterns)
        
        # D√©terminer le format demand√©
        format_type = None
        
        # Priorit√© aux formats explicitement mentionn√©s
        if "pdf" in query_lower:
            format_type = "pdf"
        elif any(word in query_lower for word in ["word", "docx", "doc"]):
            format_type = "docx"
            
        # Si aucun format sp√©cifique n'est mentionn√© mais c'est une demande de document,
        # utiliser PDF comme format par d√©faut
        if is_doc_request and not format_type:
            format_type = "pdf"  # Format par d√©faut
        
        return {
            "is_doc_request": is_doc_request,
            "format": format_type
        }
